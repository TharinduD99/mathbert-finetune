{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97669,"databundleVersionId":11615683,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import AdamW\nfrom sklearn.metrics import f1_score, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tqdm import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:13:08.319673Z","iopub.execute_input":"2025-05-08T18:13:08.320313Z","iopub.status.idle":"2025-05-08T18:13:08.324878Z","shell.execute_reply.started":"2025-05-08T18:13:08.320290Z","shell.execute_reply":"2025-05-08T18:13:08.324207Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set device (use GPU if available)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:08:17.288836Z","iopub.execute_input":"2025-05-08T18:08:17.289357Z","iopub.status.idle":"2025-05-08T18:08:17.293870Z","shell.execute_reply.started":"2025-05-08T18:08:17.289333Z","shell.execute_reply":"2025-05-08T18:08:17.293003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -------------------------------\n# 1. Load the Data\n# -------------------------------\ntrain_df = pd.read_csv(\"/kaggle/input/classification-of-math-problems-by-kasut-academy/train.csv\")  \ntest_df = pd.read_csv(\"/kaggle/input/classification-of-math-problems-by-kasut-academy/test.csv\")    \n\nprint(train_df.head())\nprint(test_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:08:20.658911Z","iopub.execute_input":"2025-05-08T18:08:20.659407Z","iopub.status.idle":"2025-05-08T18:08:20.779752Z","shell.execute_reply.started":"2025-05-08T18:08:20.659383Z","shell.execute_reply":"2025-05-08T18:08:20.779167Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train = train_df[\"Question\"].values\ny_train = train_df[\"label\"].values\nX_test  = test_df[\"Question\"].values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:08:23.138321Z","iopub.execute_input":"2025-05-08T18:08:23.138587Z","iopub.status.idle":"2025-05-08T18:08:23.143995Z","shell.execute_reply.started":"2025-05-08T18:08:23.138568Z","shell.execute_reply":"2025-05-08T18:08:23.143412Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 2. Encode Labels\nlabel_encoder = LabelEncoder()\ny_train = label_encoder.fit_transform(y_train)\n\n# 3. Tokenization\ntokenizer = BertTokenizer.from_pretrained('tbs17/MathBERT')\nMAX_LEN = 256  # Math problems are typically short\n\ndef tokenize(texts):\n    return tokenizer(\n        texts.tolist(),\n        add_special_tokens=True,\n        max_length=MAX_LEN,\n        padding='max_length',\n        truncation=True,\n        return_tensors='pt'\n    )\n\ntrain_encodings = tokenize(X_train)\ntest_encodings = tokenize(X_test)\n\n# 4. Create Datasets\ntrain_dataset = TensorDataset(\n    train_encodings['input_ids'],\n    train_encodings['attention_mask'],\n    torch.tensor(y_train)\n)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:08:32.299708Z","iopub.execute_input":"2025-05-08T18:08:32.300512Z","iopub.status.idle":"2025-05-08T18:08:48.677494Z","shell.execute_reply.started":"2025-05-08T18:08:32.300486Z","shell.execute_reply":"2025-05-08T18:08:48.676931Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 5. Initialize Model\nmodel = BertForSequenceClassification.from_pretrained(\n    'tbs17/MathBERT',\n    num_labels=len(label_encoder.classes_),\n    output_attentions=True,\n    output_hidden_states=True\n).to(device)\n\n# 6. Optimizer with Layer-wise LR Decay\ndef get_optimizer(model):\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.weight']\n    \n    # Group parameters with and without weight decay\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)]},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)]},\n    ]\n    \n    return AdamW(optimizer_grouped_parameters, lr=2e-5)\n\noptimizer = get_optimizer(model)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:15:02.438657Z","iopub.execute_input":"2025-05-08T18:15:02.438922Z","iopub.status.idle":"2025-05-08T18:15:03.833159Z","shell.execute_reply.started":"2025-05-08T18:15:02.438906Z","shell.execute_reply":"2025-05-08T18:15:03.832393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Training Loop\ndef train_epoch(model, dataloader, optimizer):\n    model.train()\n    total_loss = 0\n    preds, true_labels = [], []\n    \n    for batch in tqdm(dataloader):\n        batch = tuple(b.to(device) for b in batch)\n        inputs = {\n            'input_ids': batch[0],\n            'attention_mask': batch[1],\n            'labels': batch[2]\n        }\n        \n        optimizer.zero_grad()\n        outputs = model(**inputs)\n        loss = outputs.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n        preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n        true_labels.extend(inputs['labels'].cpu().numpy())\n    \n    return {\n        'loss': total_loss/len(dataloader),\n        'f1': f1_score(true_labels, preds, average='weighted')\n    }\n\n# 8. Evaluation\ndef evaluate(model, dataloader):\n    model.eval()\n    preds, true_labels = [], []\n    \n    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n        batch = tuple(b.to(device) for b in batch)\n        with torch.no_grad():\n            outputs = model(\n                input_ids=batch[0],\n                attention_mask=batch[1]\n            )\n        \n        preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n        true_labels.extend(batch[2].cpu().numpy())\n    \n    # Convert numeric labels to original class names\n    pred_names = label_encoder.inverse_transform(preds)\n    true_names = label_encoder.inverse_transform(true_labels)\n    \n    return {\n        'f1': f1_score(true_labels, preds, average='weighted'),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:30:52.948549Z","iopub.execute_input":"2025-05-08T18:30:52.948843Z","iopub.status.idle":"2025-05-08T18:30:52.956859Z","shell.execute_reply.started":"2025-05-08T18:30:52.948823Z","shell.execute_reply":"2025-05-08T18:30:52.956158Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# 9. Run Training\nBATCH_SIZE = 16\nEPOCHS = 10\n\n# Split train into train/val\ntrain_idx, val_idx = train_test_split(\n    np.arange(len(y_train)),\n    test_size=0.2,\n    stratify=y_train,\n    random_state=42\n)\n\ntrain_loader = DataLoader(\n    TensorDataset(\n        train_encodings['input_ids'][train_idx],\n        train_encodings['attention_mask'][train_idx],\n        torch.tensor(y_train[train_idx])\n    ),\n    batch_size=BATCH_SIZE,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    TensorDataset(\n        train_encodings['input_ids'][val_idx],\n        train_encodings['attention_mask'][val_idx],\n        torch.tensor(y_train[val_idx])\n    ),\n    batch_size=BATCH_SIZE\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:30:54.512955Z","iopub.execute_input":"2025-05-08T18:30:54.513235Z","iopub.status.idle":"2025-05-08T18:30:54.529504Z","shell.execute_reply.started":"2025-05-08T18:30:54.513217Z","shell.execute_reply":"2025-05-08T18:30:54.528915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"best_f1 = 0\nfor epoch in range(EPOCHS):\n    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n    train_metrics = train_epoch(model, train_loader, optimizer)\n    val_metrics = evaluate(model, val_loader)\n    \n    print(f\"Train Loss: {train_metrics['loss']:.4f} | Train F1: {train_metrics['f1']:.4f}\")\n    print(f\"Val F1: {val_metrics['f1']:.4f}\")\n    \n    \n    if val_metrics['f1'] > best_f1:\n        best_f1 = val_metrics['f1']\n        torch.save(model.state_dict(), 'best_mathbert_model.pth')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T18:30:56.847752Z","iopub.execute_input":"2025-05-08T18:30:56.848016Z","iopub.status.idle":"2025-05-08T19:06:16.227482Z","shell.execute_reply.started":"2025-05-08T18:30:56.847997Z","shell.execute_reply":"2025-05-08T19:06:16.226820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):\n    cm = confusion_matrix(y_true, y_pred, labels=class_names)\n    \n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', \n                xticklabels=class_names,\n                yticklabels=class_names,\n                cmap='Blues')\n    plt.title(title)\n    plt.xlabel('Predicted')\n    plt.ylabel('True')\n    plt.xticks(rotation=45)\n    plt.yticks(rotation=0)\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T19:06:21.383545Z","iopub.execute_input":"2025-05-08T19:06:21.383842Z","iopub.status.idle":"2025-05-08T19:06:21.578371Z","shell.execute_reply.started":"2025-05-08T19:06:21.383822Z","shell.execute_reply":"2025-05-08T19:06:21.577844Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_predictions(model, dataloader):\n    model.eval()\n    preds, true_labels = [], []\n    \n    for batch in dataloader:\n        batch = tuple(b.to(device) for b in batch)\n        with torch.no_grad():\n            outputs = model(\n                input_ids=batch[0],\n                attention_mask=batch[1]\n            )\n        preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n        true_labels.extend(batch[2].cpu().numpy())\n    \n    return true_labels, preds\n\n# Get predictions\ntrain_true, train_pred = get_predictions(model, train_loader)\nval_true, val_pred = get_predictions(model, val_loader)\n\n# Convert to class names\nclass_names = label_encoder.classes_\n\n# Training set\nplot_confusion_matrix(\n    label_encoder.inverse_transform(train_true),\n    label_encoder.inverse_transform(train_pred),\n    class_names,\n    title='Training Confusion Matrix'\n)\n\n# Validation set\nplot_confusion_matrix(\n    label_encoder.inverse_transform(val_true),\n    label_encoder.inverse_transform(val_pred),\n    class_names,\n    title='Validation Confusion Matrix'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T19:06:24.383612Z","iopub.execute_input":"2025-05-08T19:06:24.384125Z","iopub.status.idle":"2025-05-08T19:07:40.835572Z","shell.execute_reply.started":"2025-05-08T19:06:24.384103Z","shell.execute_reply":"2025-05-08T19:07:40.834915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 10. Final Evaluation on Test Set\ntest_dataset = TensorDataset(\n    test_encodings['input_ids'],\n    test_encodings['attention_mask']\n)\n\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n\n# 1. Load the best saved model\nmodel.load_state_dict(torch.load('best_mathbert_model.pth'))\nmodel.eval()\n\n# 2. Generate predictions\ntest_preds = []\nfor batch in tqdm(test_loader, desc=\"Predicting Test Set\"):\n    batch = tuple(b.to(device) for b in batch)\n    with torch.no_grad():\n        outputs = model(\n            input_ids=batch[0],\n            attention_mask=batch[1]\n        )\n    test_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n\n# 3. Convert to original labels\ntest_labels = label_encoder.inverse_transform(test_preds)\n\n# 4. Save predictions in required format\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"].values,  # Assuming your test_df has an 'id' column\n    \"label\": test_labels\n})\n\n# 5. Save to CSV\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"Predictions saved to submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T19:14:00.503737Z","iopub.execute_input":"2025-05-08T19:14:00.504108Z","iopub.status.idle":"2025-05-08T19:14:23.347643Z","shell.execute_reply.started":"2025-05-08T19:14:00.504085Z","shell.execute_reply":"2025-05-08T19:14:23.346934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T19:15:40.788022Z","iopub.execute_input":"2025-05-08T19:15:40.788710Z","iopub.status.idle":"2025-05-08T19:15:40.798821Z","shell.execute_reply.started":"2025-05-08T19:15:40.788688Z","shell.execute_reply":"2025-05-08T19:15:40.798322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}